I"ﬁ4<h3 id="background">Background</h3>
<p>In a <a href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html">previous note</a>, I described the problem of inferring the goodness of fit between a regional network model and the sampled output of cultural transmission on that regional network, as measured through seriations. I am now ready with the simulation and inference code to start testing the spectral similarity metric I discussed in that note across pairs and sets of regional network models.</p>
<p>Here, I describe the first such comparison.</p>
<h3 id="experiment-sc-1">Experiment: SC-1</h3>
<p>SC-1 is a simple contrast between two regional network models. A regional network model is a time-transgressive description of the interaction patterns that existed among a set of subpopulations, described by an ‚Äúinterval temporal network‚Äù representation (see <a href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2014/11/28/more-temporal-networks-python.html">note 2</a> and <a href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2014/07/28/implementing-temporal-networks-in-python.html">note 1</a> about the implementation of such models).</p>
<p>Both models are composed of 10 time slices.</p>
<p>Model #1 is called ‚Äúlinear‚Äù in the experiment directory because it should ideally yield simple, linear seriation solutions because the only thing occurring is sampling through time. At any given time, the metapopulation is composed of 64 subpopulations each. Each subpopulation is fully connected, so that any subpopulation can exchange migrants with any other, and there are no differences in edge weights (and thus migration rates). Each subpopulation in slice <span class="math inline">\(N\)</span> is the child of a random subpopulation in slice <span class="math inline">\(N-1\)</span>.</p>
<p>Model #2 is called ‚Äúlineage‚Äù in the experiment directory because it features 4 clusters of subpopulations, with 8 subpopulations per cluster. Subpopulations are fully connected within a cluster, with edge weight 10. Subpopulations are connected between subpopulations at fraction 0.1, with edge weight 1. This yields a strong tendency to exchange migrants within clusters, and at a much lower rate between clusters. For the first 4 time slices, all four clusters of subpopulations are interconnected, but in slice 4, a ‚Äúsplit‚Äù occurs, removing interconnections between two sets of clusters, leaving <span class="math inline">\({1,2}\)</span> interconnected and <span class="math inline">\({3,4}\)</span> interconnected, but no connections between these sets. The resulting clusters then evolve for 6 more time slices on separate trajectories, giving rise to a ‚Äúlineage split.‚Äù</p>
<p>Neutral cultural transmission is simulated across these two network models, with 50 simulation replicates on each model, and a common set of parameters:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode json"><code class="sourceCode json"><a class="sourceLine" id="cb1-1" title="1"><span class="fu">{</span></a>
<a class="sourceLine" id="cb1-2" title="2">    <span class="dt">&quot;theta_low&quot;</span><span class="fu">:</span> <span class="fl">0.00001</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-3" title="3">    <span class="dt">&quot;theta_high&quot;</span><span class="fu">:</span> <span class="fl">0.0001</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-4" title="4">    <span class="dt">&quot;maxinittraits&quot;</span><span class="fu">:</span> <span class="dv">5</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-5" title="5">    <span class="dt">&quot;numloci&quot;</span><span class="fu">:</span> <span class="dv">3</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-6" title="6">    <span class="dt">&quot;popsize&quot;</span><span class="fu">:</span> <span class="dv">250</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-7" title="7">    <span class="dt">&quot;simlength&quot;</span><span class="fu">:</span> <span class="dv">8000</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-8" title="8">    <span class="dt">&quot;samplefraction&quot;</span> <span class="fu">:</span> <span class="fl">0.5</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-9" title="9">    <span class="dt">&quot;migrationfraction_low&quot;</span> <span class="fu">:</span> <span class="fl">0.05</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-10" title="10">    <span class="dt">&quot;migrationfraction_high&quot;</span> <span class="fu">:</span> <span class="fl">0.1</span><span class="fu">,</span></a>
<a class="sourceLine" id="cb1-11" title="11">    <span class="dt">&quot;replicates&quot;</span> <span class="fu">:</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-12" title="12"><span class="fu">}</span></a></code></pre></div>
<p>Innovation rates and migration fractions are chosen uniformly at random from the ranges given for each simulation run. Each locus evolves randomly, but we track combinations of loci as ‚Äúclasses‚Äù in the archaeological sense of the term as our observable variables. Populations evolve for 8000 generations, giving approximately 800 transmission events per individual during a time slice (i.e., step in the regional metapopulation evolution). We can think of that as approximately monthly opportunities for copying artifacts or behavioral traits, over a lifetime of approximately 65 years.</p>
<p>The raw data from each community in a simulated network model are then aggregated over the duration that community persists, giving us a time averaged picture of the frequency of cultural traits.</p>
<p>I then perform the following sampling steps:</p>
<ol type="1">
<li><p>I take a sample of each time averaged data collection similar to the size of a typical archaeological surface collection: 500 samples are taken without replacement from each community, with probability proportional to class frequency. This yields a smaller set of classes, since many hundreds or thousands of combinations are only seen once or twice in the simulation data, and thus are very hard to measure from empirical samples.</p></li>
<li><p>I then take a sample of communities to seriate. From each of the two network models, I take temporally stratified samples, with 3 communities per time slice sampled out of the 64 present in each slice. In real sampling situations, we would not know how communities break down temporally, but we often do attempt to get samples of assemblages which cover our entire study duration. In the case of Model #1, we take a 5% sample of communities in each time slice. In the case of Model #2, given the different structure of the model, we take a 12% sample of communities in each time slice.</p></li>
<li><p>Within the overall sample from each simulation run, comprised now of the time averaged class frequencies from 30 sampled communities, I examine the classes/types themselves, and drop any types (columns) which do not have data values for at least 3 communities. This is standard pre-processing for seriation analyses (or was in the Fordian manual days of seriation analysis) since without more than 3 values, a column does not contribute to ordering the communities.</p></li>
</ol>
<p>The <a href="https://github.com/clipo/idss-seriation">IDSS Seriation</a> package is then used to seriate the stratified, filtered data files for each simulation run across the two models.</p>
<h3 id="first-classification-attempt">First Classification Attempt</h3>
<p>For a first classification attempt, I did a ‚Äúleave one out‚Äù cross validation run, in which each seriation graph was sequentially deleted from the training set of 99 seriations (one lineage graph had issues with duplicate frequencies and became stuck in frequency seriation), and the distance from the hold-out target graph to all others calculated using <a href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html">Laplacian spectral distance</a>. The label of the target graph was then predicted as the majority vote of the 5 nearest neighboring graphs. No attempt was made to tune the number of nearest neighbors using a second cross validation pass, but that will be the next experiment.</p>
<p>In general, the ability to predict the label (network model) which gave rise to the target seriation graph is decent: 76.8%.</p>
<pre><code>Classification Report:

          predicted 0  predicted 1
actual 0           41            9
actual 1           14           35
             precision    recall  f1-score   support

          0       0.75      0.82      0.78        50
          1       0.80      0.71      0.75        49

avg / total       0.77      0.77      0.77        99

Accuracy on test: 0.768</code></pre>
<p>The details of classifier accuracy seem to show that we have better ability to correctly classify seriations which result from Model #1 (‚Äúlinear‚Äù) model than seriations originating from the lineage splitting Model #2. In particular, we correctly identify instances of Model #1 82% of the time (recall), although there are clear issues with false positives. The ability to identify instances of Model #2 is worse, with a considerable number of false negatives.</p>
<p>In the next experiment, I intend to see if different values of the k-Nearest Neighbor parameter affect this accuracy, but I expect that achieving higher accuracy might require augmenting the approach. One possibility that bears exploration is to not simply use the spectral distance, but to instead use the Laplacian eigenvalues themselves (sorted in decreasing order) directly as features, in addition to other graph theoretic properties such as average degree and tree radius, and use a more traditional classifier like boosted decision trees. That will probably be my third experiment.</p>
<h3 id="second-attempt">Second Attempt</h3>
<p>In a second run, I examined the effect of the number of nearest neighbors which ‚Äúvote‚Äù on the label of the target seriation, still with leave-one-out cross validation. The results seem to indicate that (at least for these models) that best performance is 3 nearest neighbors, with accuracy worsening for 5, 7, and 9 neighbors, and then plateauing a bit for 11 and 15 neighbors. But with 3 neighbors, we achieve almost 80% accuracy, which is encouraging.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"></a>
<a class="sourceLine" id="cb3-2" title="2"></a>
<a class="sourceLine" id="cb3-3" title="3">knn <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">15</span>]</a>
<a class="sourceLine" id="cb3-4" title="4"><span class="cf">for</span> nn <span class="kw">in</span> knn:</a>
<a class="sourceLine" id="cb3-5" title="5">    gclf <span class="op">=</span> skm.GraphEigenvalueNearestNeighbors(n_neighbors<span class="op">=</span>nn)</a>
<a class="sourceLine" id="cb3-6" title="6">    test_pred <span class="op">=</span> []</a>
<a class="sourceLine" id="cb3-7" title="7">    <span class="cf">for</span> ix <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(train_graphs)):</a>
<a class="sourceLine" id="cb3-8" title="8">        train_loo_graphs, train_loo_labels, test_graph, test_label <span class="op">=</span> leave_one_out_cv(ix, train_graphs, train_labels)</a>
<a class="sourceLine" id="cb3-9" title="9">        gclf.fit(train_loo_graphs, train_loo_labels)</a>
<a class="sourceLine" id="cb3-10" title="10">        test_pred.append(gclf.predict([test_graph])[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb3-11" title="11">    <span class="bu">print</span>(<span class="st">&quot;Accuracy on test for </span><span class="sc">%s</span><span class="st"> neighbors: </span><span class="sc">%0.3f</span><span class="st">&quot;</span> <span class="op">%</span> (nn, accuracy_score(train_labels, test_pred)))</a></code></pre></div>
<p>Results:</p>
<pre><code>Accuracy on test for 1 neighbors: 0.788
Accuracy on test for 3 neighbors: 0.798
Accuracy on test for 5 neighbors: 0.768
Accuracy on test for 7 neighbors: 0.747
Accuracy on test for 9 neighbors: 0.758
Accuracy on test for 11 neighbors: 0.788
Accuracy on test for 15 neighbors: 0.788</code></pre>
<h3 id="resources">Resources</h3>
<p>The full Github repository for this and related seriation classification experiments is: <a href="https://github.com/mmadsen/experiment-seriation-classification">experiment-seriation-classification</a></p>
<p><a href="http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-seriation-classification-analysis.ipynb">Full iPython notebook</a>.</p>
<p><a href="http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-eigenvalue-classification-dataprep.ipynb">Data Preparation Notebook</a></p>
<h3 id="references-cited">References Cited</h3>
:ET